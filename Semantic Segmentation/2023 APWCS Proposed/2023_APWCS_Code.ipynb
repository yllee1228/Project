{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d82b34ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\anaconda3\\envs\\TensorFlow\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\Owner\\anaconda3\\envs\\TensorFlow\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import keract\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "# import albumentations as A\n",
    "from IPython.display import SVG\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os, re, sys, random, shutil, cv2, time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, Nadam\n",
    "from tensorflow.keras import applications, optimizers\n",
    "from tensorflow.keras.applications import InceptionResNetV2, ResNet152V2, VGG16\n",
    "from tensorflow.keras.utils import model_to_dot, plot_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, CSVLogger, LearningRateScheduler\n",
    "from tensorflow.keras.layers import Convolution2D, BatchNormalization, ReLU,LeakyReLU,Add, Activation, Conv2DTranspose, MaxPool2D\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, AveragePooling2D, UpSampling2D, Input, Dropout, ZeroPadding2D, Concatenate\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d8fe93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b37f7c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e: \n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5125f2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = \"./datasets/input/train_image2/\"\n",
    "train_masks = \"./datasets/input/train_mask2/\"\n",
    "val_images = \"./datasets/input/val_image2/\"\n",
    "val_masks = \"./datasets/input/val_mask2/\"\n",
    "test_images = \"./datasets/input/test_image2/\"\n",
    "test_masks = \"./datasets/input/test_mask2/\"\n",
    "\n",
    "num_classes = 6\n",
    "input_size = (256, 256, 3)\n",
    "image_size = (256, 256)\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4dfcf98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>r</th>\n",
       "      <th>g</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>building</td>\n",
       "      <td>60</td>\n",
       "      <td>16</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>land</td>\n",
       "      <td>132</td>\n",
       "      <td>41</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>road</td>\n",
       "      <td>110</td>\n",
       "      <td>193</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vegetation</td>\n",
       "      <td>254</td>\n",
       "      <td>221</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>water</td>\n",
       "      <td>226</td>\n",
       "      <td>169</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>unlabeled</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name    r    g    b\n",
       "0    building   60   16  152\n",
       "1        land  132   41  246\n",
       "2        road  110  193  228\n",
       "3  vegetation  254  221   58\n",
       "4       water  226  169   41\n",
       "5   unlabeled  155  155  155"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_dict_df = pd.read_csv('./datasets/input/class_dict.csv', index_col=False, skipinitialspace=True)\n",
    "class_dict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dada4924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(60, 16, 152),\n",
       "  (132, 41, 246),\n",
       "  (110, 193, 228),\n",
       "  (254, 221, 58),\n",
       "  (226, 169, 41),\n",
       "  (155, 155, 155)],\n",
       " ['building', 'land', 'road', 'vegetation', 'water', 'unlabeled'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names= list(class_dict_df.name)\n",
    "label_codes = []\n",
    "r= np.asarray(class_dict_df.r)\n",
    "g= np.asarray(class_dict_df.g)\n",
    "b= np.asarray(class_dict_df.b)\n",
    "\n",
    "for i in range(len(class_dict_df)):\n",
    "    label_codes.append(tuple([r[i], g[i], b[i]]))\n",
    "    \n",
    "label_codes, label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aea146fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "code2id = {v:k for k,v in enumerate(label_codes)}\n",
    "id2code = {k:v for k,v in enumerate(label_codes)}\n",
    "\n",
    "name2id = {v:k for k,v in enumerate(label_names)}\n",
    "id2name = {k:v for k,v in enumerate(label_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e33e8cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: (60, 16, 152),\n",
       "  1: (132, 41, 246),\n",
       "  2: (110, 193, 228),\n",
       "  3: (254, 221, 58),\n",
       "  4: (226, 169, 41),\n",
       "  5: (155, 155, 155)},\n",
       " {0: 'building',\n",
       "  1: 'land',\n",
       "  2: 'road',\n",
       "  3: 'vegetation',\n",
       "  4: 'water',\n",
       "  5: 'unlabeled'})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2code, id2name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b13139b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_to_onehot(rgb_image, colormap = id2code):\n",
    "    '''Function to one hot encode RGB mask labels\n",
    "        Inputs: \n",
    "            rgb_image - image matrix (eg. 256 x 256 x 3 dimension numpy ndarray)\n",
    "            colormap - dictionary of color to label id\n",
    "        Output: One hot encoded image of dimensions (height x width x num_classes) where num_classes = len(colormap)\n",
    "    '''\n",
    "    num_classes = len(colormap)\n",
    "    shape = rgb_image.shape[:2]+(num_classes,)\n",
    "    encoded_image = np.zeros(shape, dtype=np.int8)\n",
    "    for i, cls in enumerate(colormap):\n",
    "        encoded_image[:,:,i] = np.all(rgb_image.reshape((-1,3)) == colormap[i], axis=1).reshape(shape[:2])\n",
    "    return encoded_image\n",
    "\n",
    "\n",
    "def onehot_to_rgb(onehot, colormap = id2code):\n",
    "    '''Function to decode encoded mask labels\n",
    "        Inputs: \n",
    "            onehot - one hot encoded image matrix (height x width x num_classes)\n",
    "            colormap - dictionary of color to label id\n",
    "        Output: Decoded RGB image (height x width x 3) \n",
    "    '''\n",
    "    single_layer = np.argmax(onehot, axis=-1)\n",
    "    output = np.zeros(onehot.shape[:2] + (3,))\n",
    "    for k in colormap.keys():\n",
    "        output[single_layer==k] = colormap[k]\n",
    "    return np.uint8(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eec7359d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing only frame images, since masks contain label info\n",
    "data_gen_args = dict(rescale=1./255)\n",
    "mask_gen_args = dict()\n",
    "\n",
    "train_frames_datagen = ImageDataGenerator(**data_gen_args)\n",
    "train_masks_datagen = ImageDataGenerator(**mask_gen_args)\n",
    "val_frames_datagen = ImageDataGenerator(**data_gen_args)\n",
    "val_masks_datagen = ImageDataGenerator(**mask_gen_args)\n",
    "\n",
    "# Seed defined for aligning images and their masks\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cd61841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainAugmentGenerator(train_images_dir, train_masks_dir, seed = 1, batch_size = batch_size, target_size = image_size):\n",
    "    '''Train Image data generator\n",
    "        Inputs: \n",
    "            seed - seed provided to the flow_from_directory function to ensure aligned data flow\n",
    "            batch_size - number of images to import at a time\n",
    "            train_images_dir - train images directory\n",
    "            train_masks_dir - train masks directory\n",
    "            target_size - tuple of integers (height, width)\n",
    "            \n",
    "        Output: Decoded RGB image (height x width x 3) \n",
    "    '''\n",
    "    train_image_generator = train_frames_datagen.flow_from_directory(\n",
    "    train_images_dir,\n",
    "    batch_size = batch_size,\n",
    "    color_mode = 'rgb',\n",
    "    #color_mode = 'grayscale', #흑백사진 일 경우\n",
    "    seed = seed, \n",
    "    target_size = target_size)\n",
    "\n",
    "    train_mask_generator = train_masks_datagen.flow_from_directory(\n",
    "    train_masks_dir,\n",
    "    batch_size = batch_size, \n",
    "    seed = seed, \n",
    "    target_size = target_size)\n",
    "\n",
    "    while True:\n",
    "        X1i = train_image_generator.next()\n",
    "        X2i = train_mask_generator.next()\n",
    "        \n",
    "        #One hot encoding RGB images\n",
    "        mask_encoded = [rgb_to_onehot(X2i[0][x,:,:,:], id2code) for x in range(X2i[0].shape[0])]\n",
    "          \n",
    "        yield X1i[0], np.asarray(mask_encoded)\n",
    "\n",
    "def ValAugmentGenerator(val_images_dir, val_masks_dir, seed = 1, batch_size = batch_size, target_size = image_size):\n",
    "    '''Validation Image data generator\n",
    "        Inputs: \n",
    "            seed - seed provided to the flow_from_directory function to ensure aligned data flow\n",
    "            batch_size - number of images to import at a time\n",
    "            val_images_dir - validation images directory\n",
    "            val_masks_dir - validation masks directory\n",
    "            target_size - tuple of integers (height, width)\n",
    "            \n",
    "        Output: Decoded RGB image (height x width x 3) \n",
    "    '''\n",
    "    val_image_generator = val_frames_datagen.flow_from_directory(\n",
    "    val_images_dir,\n",
    "    batch_size = batch_size,\n",
    "    color_mode = 'rgb',\n",
    "    #color_mode = 'grayscale', #흑백사진 일 경우\n",
    "    seed = seed, \n",
    "    target_size = target_size)\n",
    "\n",
    "\n",
    "    val_mask_generator = val_masks_datagen.flow_from_directory(\n",
    "    val_masks_dir,\n",
    "    batch_size = batch_size, \n",
    "    seed = seed, \n",
    "    target_size = target_size)\n",
    "\n",
    "\n",
    "    while True:\n",
    "        X1i = val_image_generator.next()\n",
    "        X2i = val_mask_generator.next()\n",
    "        \n",
    "        #One hot encoding RGB images\n",
    "        mask_encoded = [rgb_to_onehot(X2i[0][x,:,:,:], id2code) for x in range(X2i[0].shape[0])]\n",
    "        \n",
    "#         print(f'mask_encoded: {np.asarray(mask_encoded)}')\n",
    "        \n",
    "        yield X1i[0], np.asarray(mask_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14a5d712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps_per_epoch:  18.0\n",
      "validation_steps:  22.0\n"
     ]
    }
   ],
   "source": [
    "num_train_samples = len(np.sort(os.listdir(train_images + \"images/\")))\n",
    "num_val_samples = len(np.sort(os.listdir(val_images + \"images/\")))\n",
    "steps_per_epoch = np.ceil(float(num_train_samples) / float(batch_size)/5)\n",
    "print('steps_per_epoch: ', steps_per_epoch)\n",
    "validation_steps = np.ceil(float(4 * num_val_samples) / float(batch_size)/5)\n",
    "print('validation_steps: ', validation_steps)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cce7f502",
   "metadata": {},
   "source": [
    "input_shape = (256, 256, 3)\n",
    "inputs = Input(input_shape)\n",
    "encoder = VGG16(include_top=False, weights=\"imagenet\", input_tensor=inputs)\n",
    "\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb38987e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape):\n",
    "    \n",
    "    \"\"\" Input \"\"\"\n",
    "    inputs = Input(input_shape)\n",
    "    \n",
    "    \"\"\" Transfer Learning Model \"\"\"\n",
    "    encoder = VGG16(include_top=False, weights=\"imagenet\", input_tensor=inputs)\n",
    "    \n",
    "    \"\"\" Encoder \"\"\"\n",
    "    s1 = encoder.get_layer(\"block1_conv2\").output    ## (256 x 256 x 64)\n",
    "\n",
    "    s2 = encoder.get_layer(\"block1_pool\").output      ## (128 x 128 x 64)\n",
    "\n",
    "    s3 = encoder.get_layer(\"block3_conv3\").output     ## (64 x 64 x 256)\n",
    "\n",
    "    s4 = encoder.get_layer(\"block3_pool\").output      ## (32 x 32 x 256)\n",
    "\n",
    "    \"\"\" Bridge \"\"\"\n",
    "    b1 = encoder.get_layer(\"block5_conv3\").output     ## (16 x 16 x 512)\n",
    "    \n",
    "    \"\"\" Concatenate Block \"\"\"\n",
    "    s1 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(s1)      ## (128 x 128 x 64)\n",
    "    concat1 = Concatenate()([s1, s2])\n",
    "    \n",
    "    s4 = UpSampling2D(size=(2, 2), interpolation='nearest')(s4)                ## (64 x 64 x 256)\n",
    "    concat2 = Concatenate()([s3, s4])\n",
    "    \n",
    "    concat2 = Conv2DTranspose(64, (2, 2), strides=2, padding=\"same\")(concat2)\n",
    "    last_concat = Concatenate()([concat1, concat2])\n",
    "    last_concat_128 = Convolution2D(64, 3, padding=\"same\")(last_concat)         ## (128 x 128 x 64)\n",
    "    \n",
    "    last_concat_64 = Convolution2D(128, 3, strides = (2, 2), padding=\"same\")(last_concat)\n",
    "    last_concat_64 = BatchNormalization()(last_concat_64)\n",
    "    last_concat_64 = Activation(\"relu\")(last_concat_64)                         ## 64 x 64 x 128\n",
    "    \n",
    "    last_concat_32 = Convolution2D(256, 3, strides = (2, 2), padding=\"same\")(last_concat_64)\n",
    "    last_concat_32 = BatchNormalization()(last_concat_32)\n",
    "    last_concat_32 = Activation(\"relu\")(last_concat_32)                         ## 32 x 32 x 256\n",
    "    \n",
    "    original = Conv2DTranspose(256, (2, 2), 2, padding = \"same\")(b1)            ## 32 x 32 x 256\n",
    "    original = Convolution2D(256, 3, padding=\"same\")(original)\n",
    "    original = BatchNormalization()(original)\n",
    "    original = Activation(\"relu\")(original)\n",
    "    original = Concatenate()([original, last_concat_32])\n",
    "    original = Convolution2D(256, 3, padding=\"same\")(original)\n",
    "    original = BatchNormalization()(original)\n",
    "    original = Activation(\"relu\")(original)\n",
    "    \n",
    "    original = Conv2DTranspose(128, (2, 2), 2, padding = \"same\")(original)       ## 64 x 64 x 128\n",
    "    original = Convolution2D(128, 3, padding=\"same\")(original)\n",
    "    original = BatchNormalization()(original)\n",
    "    original = Activation(\"relu\")(original)\n",
    "    original = Concatenate()([original, last_concat_64])\n",
    "    original = Convolution2D(128, 3, padding=\"same\")(original)\n",
    "    original = BatchNormalization()(original)\n",
    "    original = Activation(\"relu\")(original)\n",
    "    \n",
    "    original = Conv2DTranspose(64, (2, 2), 2, padding = \"same\")(original)        ## 128 x 128 x 64\n",
    "    original = Convolution2D(64, 3, padding=\"same\")(original)\n",
    "    original = BatchNormalization()(original)\n",
    "    original = Activation(\"relu\")(original)\n",
    "    original = Concatenate()([original, last_concat_128])\n",
    "    original = Convolution2D(64, 3, padding=\"same\")(original)\n",
    "    original = BatchNormalization()(original)\n",
    "    original = Activation(\"relu\")(original)\n",
    "    \n",
    "    original = Conv2DTranspose(32, (2, 2), 2, padding = \"same\")(original)        ## 256 x 256 x 32\n",
    "    original = Convolution2D(32, 3, padding=\"same\")(original)\n",
    "    original = BatchNormalization()(original)\n",
    "    original = Activation(\"relu\")(original)\n",
    "    original = Convolution2D(32, 3, padding=\"same\")(original)\n",
    "    original = BatchNormalization()(original)\n",
    "    \n",
    "    outputs = Activation(\"relu\")(original)\n",
    "    \n",
    "    return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6689297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_conv_module(input_shape):\n",
    "    inputs, X = build_model(input_shape)\n",
    "    X = Convolution2D(filters=3, kernel_size=3, padding='same')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    # dropout = Dropout(0.3)(X)\n",
    "    outputs = Convolution2D(num_classes, 1, padding=\"same\", activation=\"softmax\")(X)\n",
    "    \n",
    "    model = Model(inputs, outputs, name=\"vgg16\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1de4c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    # Flatten\n",
    "    y_true_f = tf.reshape(y_true, [-1])\n",
    "    y_pred_f = tf.reshape(y_pred, [-1])\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "    score = (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n",
    "    return score\n",
    "\n",
    "# ship detection = class 2개일 때 클래스 불균형 확인차 iou 2개(metrics=[iou0, iou1, dice_coef, \"accuracy\"]) 설정\n",
    "# 다중 클래스일 경우, mean iou 사용\n",
    "\n",
    "def mean_iou(y_true, y_pred, num_classes = 6):\n",
    "    \"\"\"\n",
    "    Calculate the mean IoU across all classes.\n",
    "    Args:\n",
    "        y_true: the expected y values as a one-hot encoded tensor\n",
    "        y_pred: the predicted y values as a one-hot or softmax tensor\n",
    "        num_classes: total number of classes\n",
    "    Returns:\n",
    "        mean IoU across all classes\n",
    "    \"\"\"\n",
    "    ious = []\n",
    "    y_true = K.argmax(y_true, axis=-1)  # 라벨 인코딩으로 변환\n",
    "    y_pred = K.argmax(y_pred, axis=-1)  # 예측값 인코딩으로 변환\n",
    "\n",
    "    for label in range(num_classes):\n",
    "        y_true_label = K.cast(K.equal(y_true, label), K.floatx())\n",
    "        y_pred_label = K.cast(K.equal(y_pred, label), K.floatx())\n",
    "        \n",
    "        # Intersection과 Union 계산\n",
    "        intersection = K.sum(y_true_label * y_pred_label)\n",
    "        union = K.sum(y_true_label) + K.sum(y_pred_label) - intersection\n",
    "        \n",
    "        # IoU 계산 (divide by zero 방지)\n",
    "        iou = K.switch(K.equal(union, 0), 1.0, intersection / union)\n",
    "        ious.append(iou)\n",
    "    \n",
    "    # 모든 클래스의 IoU 평균\n",
    "    mean_iou = K.mean(K.stack(ious), axis=0)\n",
    "    \n",
    "    return mean_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "595c9750",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = last_conv_module(input_shape = (256, 256, 3))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3a4ba908",
   "metadata": {},
   "source": [
    "# 비교 실험 Semantic Segmentation\n",
    "import segmentation_models as sm\n",
    "\n",
    "model = sm.Unet('inceptionresnetv2', input_shape=(256, 256, 3), classes=6, activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcdc9728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 256, 256, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " block1_conv1 (Conv2D)          (None, 256, 256, 64  1792        ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block1_conv2 (Conv2D)          (None, 256, 256, 64  36928       ['block1_conv1[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block1_pool (MaxPooling2D)     (None, 128, 128, 64  0           ['block1_conv2[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block2_conv1 (Conv2D)          (None, 128, 128, 12  73856       ['block1_pool[0][0]']            \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " block2_conv2 (Conv2D)          (None, 128, 128, 12  147584      ['block2_conv1[0][0]']           \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " block2_pool (MaxPooling2D)     (None, 64, 64, 128)  0           ['block2_conv2[0][0]']           \n",
      "                                                                                                  \n",
      " block3_conv1 (Conv2D)          (None, 64, 64, 256)  295168      ['block2_pool[0][0]']            \n",
      "                                                                                                  \n",
      " block3_conv2 (Conv2D)          (None, 64, 64, 256)  590080      ['block3_conv1[0][0]']           \n",
      "                                                                                                  \n",
      " block3_conv3 (Conv2D)          (None, 64, 64, 256)  590080      ['block3_conv2[0][0]']           \n",
      "                                                                                                  \n",
      " block3_pool (MaxPooling2D)     (None, 32, 32, 256)  0           ['block3_conv3[0][0]']           \n",
      "                                                                                                  \n",
      " block4_conv1 (Conv2D)          (None, 32, 32, 512)  1180160     ['block3_pool[0][0]']            \n",
      "                                                                                                  \n",
      " block4_conv2 (Conv2D)          (None, 32, 32, 512)  2359808     ['block4_conv1[0][0]']           \n",
      "                                                                                                  \n",
      " up_sampling2d (UpSampling2D)   (None, 64, 64, 256)  0           ['block3_pool[0][0]']            \n",
      "                                                                                                  \n",
      " block4_conv3 (Conv2D)          (None, 32, 32, 512)  2359808     ['block4_conv2[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 128, 128, 64  0           ['block1_conv2[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 64, 64, 512)  0           ['block3_conv3[0][0]',           \n",
      "                                                                  'up_sampling2d[0][0]']          \n",
      "                                                                                                  \n",
      " block4_pool (MaxPooling2D)     (None, 16, 16, 512)  0           ['block4_conv3[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 128, 128, 12  0           ['max_pooling2d[0][0]',          \n",
      "                                8)                                'block1_pool[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_transpose (Conv2DTransp  (None, 128, 128, 64  131136     ['concatenate_1[0][0]']          \n",
      " ose)                           )                                                                 \n",
      "                                                                                                  \n",
      " block5_conv1 (Conv2D)          (None, 16, 16, 512)  2359808     ['block4_pool[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 128, 128, 19  0           ['concatenate[0][0]',            \n",
      "                                2)                                'conv2d_transpose[0][0]']       \n",
      "                                                                                                  \n",
      " block5_conv2 (Conv2D)          (None, 16, 16, 512)  2359808     ['block5_conv1[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 64, 64, 128)  221312      ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " block5_conv3 (Conv2D)          (None, 16, 16, 512)  2359808     ['block5_conv2[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 64, 64, 128)  512        ['conv2d_1[0][0]']               \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2DTran  (None, 32, 32, 256)  524544     ['block5_conv3[0][0]']           \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 64, 64, 128)  0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 32, 32, 256)  590080      ['conv2d_transpose_1[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 32, 32, 256)  295168      ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 32, 32, 256)  1024       ['conv2d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32, 32, 256)  1024       ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 32, 32, 256)  0           ['batch_normalization_2[0][0]']  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 32, 32, 256)  0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 32, 32, 512)  0           ['activation_2[0][0]',           \n",
      "                                                                  'activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 32, 32, 256)  1179904     ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 32, 32, 256)  1024       ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 32, 32, 256)  0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2DTran  (None, 64, 64, 128)  131200     ['activation_3[0][0]']           \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 64, 64, 128)  147584      ['conv2d_transpose_2[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 64, 64, 128)  512        ['conv2d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 64, 64, 128)  0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 64, 64, 256)  0           ['activation_4[0][0]',           \n",
      "                                                                  'activation[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 64, 64, 128)  295040      ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 64, 64, 128)  512        ['conv2d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 64, 64, 128)  0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_transpose_3 (Conv2DTran  (None, 128, 128, 64  32832      ['activation_5[0][0]']           \n",
      " spose)                         )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 128, 128, 64  36928       ['conv2d_transpose_3[0][0]']     \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 128, 128, 64  256        ['conv2d_7[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 128, 128, 64  0           ['batch_normalization_6[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 128, 128, 64  110656      ['concatenate_2[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 128, 128, 12  0           ['activation_6[0][0]',           \n",
      "                                8)                                'conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 128, 128, 64  73792       ['concatenate_5[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 128, 128, 64  256        ['conv2d_8[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 128, 128, 64  0           ['batch_normalization_7[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_transpose_4 (Conv2DTran  (None, 256, 256, 32  8224       ['activation_7[0][0]']           \n",
      " spose)                         )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 256, 256, 32  9248        ['conv2d_transpose_4[0][0]']     \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 256, 256, 32  128        ['conv2d_9[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 256, 256, 32  0           ['batch_normalization_8[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 256, 256, 32  9248        ['activation_8[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 256, 256, 32  128        ['conv2d_10[0][0]']              \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 256, 256, 32  0           ['batch_normalization_9[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 256, 256, 3)  867         ['activation_9[0][0]']           \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch_normalization_10 (BatchN  (None, 256, 256, 3)  12         ['conv2d_11[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 256, 256, 6)  24          ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 18,517,863\n",
      "Trainable params: 18,515,169\n",
      "Non-trainable params: 2,694\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=Adam(\n",
    "    learning_rate = 0.001), loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False), metrics=[mean_iou, dice_coef, \"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b4055817",
   "metadata": {},
   "source": [
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
    "plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True, expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd3c8b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay(lr0, s, warmup_epochs=10, min_lr=1e-6):\n",
    "    \"\"\"\n",
    "    Exponential Decay Learning Rate Scheduler with Warmup and Minimum Learning Rate.\n",
    "    \n",
    "    Args:\n",
    "        lr0: float, initial learning rate.\n",
    "        s: float, decay step (controls the rate of decay).\n",
    "        warmup_epochs: int, number of epochs to keep the initial learning rate constant.\n",
    "        min_lr: float, minimum learning rate after decay.\n",
    "    \n",
    "    Returns:\n",
    "        A function that computes the learning rate based on the current epoch.\n",
    "    \"\"\"\n",
    "    def exponential_decay_fn(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            return lr0  # Warmup: Keep initial learning rate for `warmup_epochs`\n",
    "        lr = lr0 * 0.1 ** ((epoch - warmup_epochs) / s)\n",
    "        return max(lr, min_lr)  # Ensure learning rate does not go below `min_lr`\n",
    "    \n",
    "    return exponential_decay_fn\n",
    "\n",
    "exponential_decay_fn = exponential_decay(0.001, 60)\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(\n",
    "    exponential_decay_fn,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath = 'APWCS_model.h5',\n",
    "    save_best_only = True, \n",
    "#   save_weights_only = False,\n",
    "    monitor = 'val_loss', \n",
    "    mode = 'auto', \n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "earlystop = EarlyStopping(\n",
    "    monitor = 'val_loss', \n",
    "    min_delta = 0.001, \n",
    "    patience = 12, \n",
    "    mode = 'auto', \n",
    "    verbose = 1,\n",
    "    restore_best_weights = True\n",
    ")\n",
    "\n",
    "csvlogger = CSVLogger(\n",
    "    filename= \"APWCS_model.csv\",\n",
    "    separator = \",\",\n",
    "    append = False\n",
    ")\n",
    "\n",
    "callbacks = [checkpoint, earlystop, csvlogger, lr_scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be55284e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 710 images belonging to 1 classes.\n",
      "Found 710 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "a, b = next(TrainAugmentGenerator(train_images_dir = train_images, train_masks_dir = train_masks, batch_size = batch_size, target_size = image_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0fede1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 710 images belonging to 1 classes.\n",
      "Found 710 images belonging to 1 classes.\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 1/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.4946 - mean_iou: 0.2309 - dice_coef: 0.2571 - accuracy: 0.4690Found 213 images belonging to 1 classes.\n",
      "Found 213 images belonging to 1 classes.\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 7.85350, saving model to APWCS_model.h5\n",
      "18/18 [==============================] - 10s 239ms/step - loss: 1.4946 - mean_iou: 0.2309 - dice_coef: 0.2571 - accuracy: 0.4690 - val_loss: 7.8535 - val_mean_iou: 0.0277 - val_dice_coef: 0.1231 - val_accuracy: 0.1206 - lr: 0.0010\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 2/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2257 - mean_iou: 0.3650 - dice_coef: 0.3217 - accuracy: 0.6930\n",
      "Epoch 2: val_loss did not improve from 7.85350\n",
      "18/18 [==============================] - 4s 224ms/step - loss: 1.2257 - mean_iou: 0.3650 - dice_coef: 0.3217 - accuracy: 0.6930 - val_loss: 67.4216 - val_mean_iou: 0.0170 - val_dice_coef: 0.1011 - val_accuracy: 0.1035 - lr: 0.0010\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 3/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.2081 - mean_iou: 0.3458 - dice_coef: 0.3355 - accuracy: 0.6973\n",
      "Epoch 3: val_loss did not improve from 7.85350\n",
      "18/18 [==============================] - 3s 194ms/step - loss: 1.2081 - mean_iou: 0.3458 - dice_coef: 0.3355 - accuracy: 0.6973 - val_loss: 44.8245 - val_mean_iou: 0.0193 - val_dice_coef: 0.1143 - val_accuracy: 0.1157 - lr: 0.0010\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 4/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1873 - mean_iou: 0.3381 - dice_coef: 0.3518 - accuracy: 0.6794\n",
      "Epoch 4: val_loss did not improve from 7.85350\n",
      "18/18 [==============================] - 3s 191ms/step - loss: 1.1873 - mean_iou: 0.3381 - dice_coef: 0.3518 - accuracy: 0.6794 - val_loss: 7.9598 - val_mean_iou: 0.1321 - val_dice_coef: 0.2014 - val_accuracy: 0.2725 - lr: 0.0010\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 5/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.0942 - mean_iou: 0.3421 - dice_coef: 0.3802 - accuracy: 0.7080\n",
      "Epoch 5: val_loss improved from 7.85350 to 2.88982, saving model to APWCS_model.h5\n",
      "18/18 [==============================] - 6s 339ms/step - loss: 1.0942 - mean_iou: 0.3421 - dice_coef: 0.3802 - accuracy: 0.7080 - val_loss: 2.8898 - val_mean_iou: 0.1713 - val_dice_coef: 0.2832 - val_accuracy: 0.3495 - lr: 0.0010\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 6/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.0680 - mean_iou: 0.3419 - dice_coef: 0.3975 - accuracy: 0.7148\n",
      "Epoch 6: val_loss improved from 2.88982 to 1.23773, saving model to APWCS_model.h5\n",
      "18/18 [==============================] - 4s 209ms/step - loss: 1.0680 - mean_iou: 0.3419 - dice_coef: 0.3975 - accuracy: 0.7148 - val_loss: 1.2377 - val_mean_iou: 0.2749 - val_dice_coef: 0.4135 - val_accuracy: 0.6434 - lr: 0.0010\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 7/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.0405 - mean_iou: 0.3471 - dice_coef: 0.4111 - accuracy: 0.7078\n",
      "Epoch 7: val_loss did not improve from 1.23773\n",
      "18/18 [==============================] - 3s 197ms/step - loss: 1.0405 - mean_iou: 0.3471 - dice_coef: 0.4111 - accuracy: 0.7078 - val_loss: 1.3067 - val_mean_iou: 0.2166 - val_dice_coef: 0.3464 - val_accuracy: 0.5327 - lr: 0.0010\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 8/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.0055 - mean_iou: 0.3550 - dice_coef: 0.4312 - accuracy: 0.7255\n",
      "Epoch 8: val_loss did not improve from 1.23773\n",
      "18/18 [==============================] - 3s 194ms/step - loss: 1.0055 - mean_iou: 0.3550 - dice_coef: 0.4312 - accuracy: 0.7255 - val_loss: 1.5111 - val_mean_iou: 0.2847 - val_dice_coef: 0.3938 - val_accuracy: 0.5956 - lr: 0.0010\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 9/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.9870 - mean_iou: 0.3520 - dice_coef: 0.4421 - accuracy: 0.7141\n",
      "Epoch 9: val_loss improved from 1.23773 to 0.96919, saving model to APWCS_model.h5\n",
      "18/18 [==============================] - 4s 209ms/step - loss: 0.9870 - mean_iou: 0.3520 - dice_coef: 0.4421 - accuracy: 0.7141 - val_loss: 0.9692 - val_mean_iou: 0.3326 - val_dice_coef: 0.4677 - val_accuracy: 0.7082 - lr: 0.0010\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 10/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.9661 - mean_iou: 0.3488 - dice_coef: 0.4519 - accuracy: 0.7136\n",
      "Epoch 10: val_loss improved from 0.96919 to 0.96661, saving model to APWCS_model.h5\n",
      "18/18 [==============================] - 4s 207ms/step - loss: 0.9661 - mean_iou: 0.3488 - dice_coef: 0.4519 - accuracy: 0.7136 - val_loss: 0.9666 - val_mean_iou: 0.3576 - val_dice_coef: 0.4609 - val_accuracy: 0.7357 - lr: 0.0010\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 11/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.0005 - mean_iou: 0.3367 - dice_coef: 0.4507 - accuracy: 0.6895\n",
      "Epoch 11: val_loss did not improve from 0.96661\n",
      "18/18 [==============================] - 3s 192ms/step - loss: 1.0005 - mean_iou: 0.3367 - dice_coef: 0.4507 - accuracy: 0.6895 - val_loss: 1.0601 - val_mean_iou: 0.3270 - val_dice_coef: 0.4994 - val_accuracy: 0.6929 - lr: 0.0010\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 0.0009623506263980886.\n",
      "Epoch 12/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.9433 - mean_iou: 0.3379 - dice_coef: 0.4805 - accuracy: 0.7066\n",
      "Epoch 12: val_loss improved from 0.96661 to 0.94457, saving model to APWCS_model.h5\n",
      "18/18 [==============================] - 4s 208ms/step - loss: 0.9433 - mean_iou: 0.3379 - dice_coef: 0.4805 - accuracy: 0.7066 - val_loss: 0.9446 - val_mean_iou: 0.3196 - val_dice_coef: 0.5596 - val_accuracy: 0.7046 - lr: 9.6235e-04\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 0.0009261187281287935.\n",
      "Epoch 13/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.9126 - mean_iou: 0.3549 - dice_coef: 0.4901 - accuracy: 0.7332\n",
      "Epoch 13: val_loss did not improve from 0.94457\n",
      "18/18 [==============================] - 3s 190ms/step - loss: 0.9126 - mean_iou: 0.3549 - dice_coef: 0.4901 - accuracy: 0.7332 - val_loss: 1.1229 - val_mean_iou: 0.3008 - val_dice_coef: 0.5338 - val_accuracy: 0.6750 - lr: 9.2612e-04\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 0.0008912509381337456.\n",
      "Epoch 14/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.9068 - mean_iou: 0.3470 - dice_coef: 0.4921 - accuracy: 0.7216\n",
      "Epoch 14: val_loss did not improve from 0.94457\n",
      "18/18 [==============================] - 3s 192ms/step - loss: 0.9068 - mean_iou: 0.3470 - dice_coef: 0.4921 - accuracy: 0.7216 - val_loss: 1.1694 - val_mean_iou: 0.2693 - val_dice_coef: 0.4522 - val_accuracy: 0.5626 - lr: 8.9125e-04\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 0.0008576958985908942.\n",
      "Epoch 15/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.8731 - mean_iou: 0.3685 - dice_coef: 0.5078 - accuracy: 0.7363\n",
      "Epoch 15: val_loss improved from 0.94457 to 0.86148, saving model to APWCS_model.h5\n",
      "18/18 [==============================] - 4s 206ms/step - loss: 0.8731 - mean_iou: 0.3685 - dice_coef: 0.5078 - accuracy: 0.7363 - val_loss: 0.8615 - val_mean_iou: 0.3705 - val_dice_coef: 0.5120 - val_accuracy: 0.7599 - lr: 8.5770e-04\n",
      "\n",
      "Epoch 16: LearningRateScheduler setting learning rate to 0.0008254041852680184.\n",
      "Epoch 16/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.8286 - mean_iou: 0.3924 - dice_coef: 0.5300 - accuracy: 0.7484\n",
      "Epoch 16: val_loss did not improve from 0.86148\n",
      "18/18 [==============================] - 3s 191ms/step - loss: 0.8286 - mean_iou: 0.3924 - dice_coef: 0.5300 - accuracy: 0.7484 - val_loss: 1.0333 - val_mean_iou: 0.3019 - val_dice_coef: 0.4720 - val_accuracy: 0.6696 - lr: 8.2540e-04\n",
      "\n",
      "Epoch 17: LearningRateScheduler setting learning rate to 0.0007943282347242815.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.8887 - mean_iou: 0.3576 - dice_coef: 0.5173 - accuracy: 0.7154\n",
      "Epoch 17: val_loss did not improve from 0.86148\n",
      "18/18 [==============================] - 3s 190ms/step - loss: 0.8887 - mean_iou: 0.3576 - dice_coef: 0.5173 - accuracy: 0.7154 - val_loss: 0.9599 - val_mean_iou: 0.2634 - val_dice_coef: 0.5532 - val_accuracy: 0.6691 - lr: 7.9433e-04\n",
      "\n",
      "Epoch 18: LearningRateScheduler setting learning rate to 0.0007644222742526003.\n",
      "Epoch 18/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.8257 - mean_iou: 0.3803 - dice_coef: 0.5375 - accuracy: 0.7385\n",
      "Epoch 18: val_loss improved from 0.86148 to 0.79447, saving model to APWCS_model.h5\n",
      "18/18 [==============================] - 4s 207ms/step - loss: 0.8257 - mean_iou: 0.3803 - dice_coef: 0.5375 - accuracy: 0.7385 - val_loss: 0.7945 - val_mean_iou: 0.3590 - val_dice_coef: 0.5922 - val_accuracy: 0.7385 - lr: 7.6442e-04\n",
      "\n",
      "Epoch 19: LearningRateScheduler setting learning rate to 0.0007356422544596414.\n",
      "Epoch 19/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.8507 - mean_iou: 0.3722 - dice_coef: 0.5355 - accuracy: 0.7317\n",
      "Epoch 19: val_loss did not improve from 0.79447\n",
      "18/18 [==============================] - 3s 196ms/step - loss: 0.8507 - mean_iou: 0.3722 - dice_coef: 0.5355 - accuracy: 0.7317 - val_loss: 0.8759 - val_mean_iou: 0.3609 - val_dice_coef: 0.5064 - val_accuracy: 0.7348 - lr: 7.3564e-04\n",
      "\n",
      "Epoch 20: LearningRateScheduler setting learning rate to 0.0007079457843841379.\n",
      "Epoch 20/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.8096 - mean_iou: 0.3865 - dice_coef: 0.5442 - accuracy: 0.7514\n",
      "Epoch 20: val_loss did not improve from 0.79447\n",
      "18/18 [==============================] - 3s 190ms/step - loss: 0.8096 - mean_iou: 0.3865 - dice_coef: 0.5442 - accuracy: 0.7514 - val_loss: 0.9416 - val_mean_iou: 0.2673 - val_dice_coef: 0.5580 - val_accuracy: 0.6784 - lr: 7.0795e-04\n",
      "\n",
      "Epoch 21: LearningRateScheduler setting learning rate to 0.0006812920690579614.\n",
      "Epoch 21/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.8410 - mean_iou: 0.3653 - dice_coef: 0.5401 - accuracy: 0.7315\n",
      "Epoch 21: val_loss did not improve from 0.79447\n",
      "18/18 [==============================] - 3s 192ms/step - loss: 0.8410 - mean_iou: 0.3653 - dice_coef: 0.5401 - accuracy: 0.7315 - val_loss: 0.8925 - val_mean_iou: 0.3487 - val_dice_coef: 0.5461 - val_accuracy: 0.7037 - lr: 6.8129e-04\n",
      "\n",
      "Epoch 22: LearningRateScheduler setting learning rate to 0.0006556418494179789.\n",
      "Epoch 22/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.8422 - mean_iou: 0.3678 - dice_coef: 0.5420 - accuracy: 0.7184\n",
      "Epoch 22: val_loss improved from 0.79447 to 0.79049, saving model to APWCS_model.h5\n",
      "18/18 [==============================] - 4s 209ms/step - loss: 0.8422 - mean_iou: 0.3678 - dice_coef: 0.5420 - accuracy: 0.7184 - val_loss: 0.7905 - val_mean_iou: 0.3726 - val_dice_coef: 0.5793 - val_accuracy: 0.7370 - lr: 6.5564e-04\n",
      "\n",
      "Epoch 23: LearningRateScheduler setting learning rate to 0.0006309573444801933.\n",
      "Epoch 23/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.8139 - mean_iou: 0.3697 - dice_coef: 0.5559 - accuracy: 0.7344\n",
      "Epoch 23: val_loss improved from 0.79049 to 0.75103, saving model to APWCS_model.h5\n",
      "18/18 [==============================] - 4s 221ms/step - loss: 0.8139 - mean_iou: 0.3697 - dice_coef: 0.5559 - accuracy: 0.7344 - val_loss: 0.7510 - val_mean_iou: 0.3725 - val_dice_coef: 0.5924 - val_accuracy: 0.7512 - lr: 6.3096e-04\n",
      "\n",
      "Epoch 24: LearningRateScheduler setting learning rate to 0.0006072021956909885.\n",
      "Epoch 24/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.7648 - mean_iou: 0.3951 - dice_coef: 0.5743 - accuracy: 0.7539\n",
      "Epoch 24: val_loss improved from 0.75103 to 0.68388, saving model to APWCS_model.h5\n",
      "18/18 [==============================] - 4s 215ms/step - loss: 0.7648 - mean_iou: 0.3951 - dice_coef: 0.5743 - accuracy: 0.7539 - val_loss: 0.6839 - val_mean_iou: 0.4265 - val_dice_coef: 0.6095 - val_accuracy: 0.7879 - lr: 6.0720e-04\n",
      "\n",
      "Epoch 25: LearningRateScheduler setting learning rate to 0.0005843414133735176.\n",
      "Epoch 25/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.7535 - mean_iou: 0.3861 - dice_coef: 0.5823 - accuracy: 0.7640\n",
      "Epoch 25: val_loss did not improve from 0.68388\n",
      "18/18 [==============================] - 3s 197ms/step - loss: 0.7535 - mean_iou: 0.3861 - dice_coef: 0.5823 - accuracy: 0.7640 - val_loss: 0.9000 - val_mean_iou: 0.3291 - val_dice_coef: 0.5454 - val_accuracy: 0.7002 - lr: 5.8434e-04\n",
      "\n",
      "Epoch 26: LearningRateScheduler setting learning rate to 0.0005623413251903491.\n",
      "Epoch 26/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.7881 - mean_iou: 0.3899 - dice_coef: 0.5661 - accuracy: 0.7383\n",
      "Epoch 26: val_loss did not improve from 0.68388\n",
      "18/18 [==============================] - 3s 196ms/step - loss: 0.7881 - mean_iou: 0.3899 - dice_coef: 0.5661 - accuracy: 0.7383 - val_loss: 0.7947 - val_mean_iou: 0.3866 - val_dice_coef: 0.5614 - val_accuracy: 0.7521 - lr: 5.6234e-04\n",
      "\n",
      "Epoch 27: LearningRateScheduler setting learning rate to 0.0005411695265464636.\n",
      "Epoch 27/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.7190 - mean_iou: 0.4078 - dice_coef: 0.5952 - accuracy: 0.7697\n",
      "Epoch 27: val_loss did not improve from 0.68388\n",
      "18/18 [==============================] - 3s 196ms/step - loss: 0.7190 - mean_iou: 0.4078 - dice_coef: 0.5952 - accuracy: 0.7697 - val_loss: 0.7516 - val_mean_iou: 0.3938 - val_dice_coef: 0.5775 - val_accuracy: 0.7627 - lr: 5.4117e-04\n",
      "\n",
      "Epoch 28: LearningRateScheduler setting learning rate to 0.0005207948328595464.\n",
      "Epoch 28/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.7651 - mean_iou: 0.3794 - dice_coef: 0.5825 - accuracy: 0.7445\n",
      "Epoch 28: val_loss did not improve from 0.68388\n",
      "18/18 [==============================] - 3s 196ms/step - loss: 0.7651 - mean_iou: 0.3794 - dice_coef: 0.5825 - accuracy: 0.7445 - val_loss: 0.8328 - val_mean_iou: 0.3781 - val_dice_coef: 0.5525 - val_accuracy: 0.7301 - lr: 5.2079e-04\n",
      "\n",
      "Epoch 29: LearningRateScheduler setting learning rate to 0.0005011872336272724.\n",
      "Epoch 29/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.7722 - mean_iou: 0.3872 - dice_coef: 0.5784 - accuracy: 0.7488\n",
      "Epoch 29: val_loss did not improve from 0.68388\n",
      "18/18 [==============================] - 3s 191ms/step - loss: 0.7722 - mean_iou: 0.3872 - dice_coef: 0.5784 - accuracy: 0.7488 - val_loss: 0.7356 - val_mean_iou: 0.4168 - val_dice_coef: 0.5893 - val_accuracy: 0.7669 - lr: 5.0119e-04\n",
      "\n",
      "Epoch 30: LearningRateScheduler setting learning rate to 0.0004823178482239307.\n",
      "Epoch 30/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.7443 - mean_iou: 0.4019 - dice_coef: 0.5818 - accuracy: 0.7561\n",
      "Epoch 30: val_loss did not improve from 0.68388\n",
      "18/18 [==============================] - 3s 192ms/step - loss: 0.7443 - mean_iou: 0.4019 - dice_coef: 0.5818 - accuracy: 0.7561 - val_loss: 0.7100 - val_mean_iou: 0.4202 - val_dice_coef: 0.5857 - val_accuracy: 0.7819 - lr: 4.8232e-04\n",
      "\n",
      "Epoch 31: LearningRateScheduler setting learning rate to 0.00046415888336127795.\n",
      "Epoch 31/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.7749 - mean_iou: 0.3993 - dice_coef: 0.5759 - accuracy: 0.7456\n",
      "Epoch 31: val_loss did not improve from 0.68388\n",
      "18/18 [==============================] - 3s 198ms/step - loss: 0.7749 - mean_iou: 0.3993 - dice_coef: 0.5759 - accuracy: 0.7456 - val_loss: 0.7268 - val_mean_iou: 0.4174 - val_dice_coef: 0.5772 - val_accuracy: 0.7721 - lr: 4.6416e-04\n",
      "\n",
      "Epoch 32: LearningRateScheduler setting learning rate to 0.00044668359215096316.\n",
      "Epoch 32/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.7283 - mean_iou: 0.4173 - dice_coef: 0.5982 - accuracy: 0.7655\n",
      "Epoch 32: val_loss improved from 0.68388 to 0.67456, saving model to APWCS_model.h5\n",
      "18/18 [==============================] - 4s 210ms/step - loss: 0.7283 - mean_iou: 0.4173 - dice_coef: 0.5982 - accuracy: 0.7655 - val_loss: 0.6746 - val_mean_iou: 0.4050 - val_dice_coef: 0.6273 - val_accuracy: 0.7736 - lr: 4.4668e-04\n",
      "\n",
      "Epoch 33: LearningRateScheduler setting learning rate to 0.00042986623470822773.\n",
      "Epoch 33/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.7158 - mean_iou: 0.4182 - dice_coef: 0.6017 - accuracy: 0.7669\n",
      "Epoch 33: val_loss did not improve from 0.67456\n",
      "18/18 [==============================] - 3s 192ms/step - loss: 0.7158 - mean_iou: 0.4182 - dice_coef: 0.6017 - accuracy: 0.7669 - val_loss: 0.7499 - val_mean_iou: 0.3908 - val_dice_coef: 0.5950 - val_accuracy: 0.7497 - lr: 4.2987e-04\n",
      "\n",
      "Epoch 34: LearningRateScheduler setting learning rate to 0.0004136820402388507.\n",
      "Epoch 34/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.7048 - mean_iou: 0.4148 - dice_coef: 0.6078 - accuracy: 0.7721\n",
      "Epoch 34: val_loss did not improve from 0.67456\n",
      "18/18 [==============================] - 3s 194ms/step - loss: 0.7048 - mean_iou: 0.4148 - dice_coef: 0.6078 - accuracy: 0.7721 - val_loss: 0.6787 - val_mean_iou: 0.4357 - val_dice_coef: 0.6054 - val_accuracy: 0.7881 - lr: 4.1368e-04\n",
      "\n",
      "Epoch 35: LearningRateScheduler setting learning rate to 0.00039810717055349724.\n",
      "Epoch 35/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.6745 - mean_iou: 0.4283 - dice_coef: 0.6178 - accuracy: 0.7914\n",
      "Epoch 35: val_loss did not improve from 0.67456\n",
      "18/18 [==============================] - 3s 193ms/step - loss: 0.6745 - mean_iou: 0.4283 - dice_coef: 0.6178 - accuracy: 0.7914 - val_loss: 0.6965 - val_mean_iou: 0.3926 - val_dice_coef: 0.6372 - val_accuracy: 0.7642 - lr: 3.9811e-04\n",
      "\n",
      "Epoch 36: LearningRateScheduler setting learning rate to 0.00038311868495572875.\n",
      "Epoch 36/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.6798 - mean_iou: 0.4384 - dice_coef: 0.6140 - accuracy: 0.7815\n",
      "Epoch 36: val_loss improved from 0.67456 to 0.66932, saving model to APWCS_model.h5\n",
      "18/18 [==============================] - 4s 215ms/step - loss: 0.6798 - mean_iou: 0.4384 - dice_coef: 0.6140 - accuracy: 0.7815 - val_loss: 0.6693 - val_mean_iou: 0.4491 - val_dice_coef: 0.6158 - val_accuracy: 0.7920 - lr: 3.8312e-04\n",
      "\n",
      "Epoch 37: LearningRateScheduler setting learning rate to 0.00036869450645195755.\n",
      "Epoch 37/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.7036 - mean_iou: 0.4418 - dice_coef: 0.6129 - accuracy: 0.7851\n",
      "Epoch 37: val_loss improved from 0.66932 to 0.65260, saving model to APWCS_model.h5\n",
      "18/18 [==============================] - 4s 212ms/step - loss: 0.7036 - mean_iou: 0.4418 - dice_coef: 0.6129 - accuracy: 0.7851 - val_loss: 0.6526 - val_mean_iou: 0.4112 - val_dice_coef: 0.6520 - val_accuracy: 0.7865 - lr: 3.6869e-04\n",
      "\n",
      "Epoch 38: LearningRateScheduler setting learning rate to 0.0003548133892335755.\n",
      "Epoch 38/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.6971 - mean_iou: 0.4414 - dice_coef: 0.6093 - accuracy: 0.7746\n",
      "Epoch 38: val_loss did not improve from 0.65260\n",
      "18/18 [==============================] - 3s 193ms/step - loss: 0.6971 - mean_iou: 0.4414 - dice_coef: 0.6093 - accuracy: 0.7746 - val_loss: 0.6724 - val_mean_iou: 0.4457 - val_dice_coef: 0.6356 - val_accuracy: 0.7797 - lr: 3.5481e-04\n",
      "\n",
      "Epoch 39: LearningRateScheduler setting learning rate to 0.0003414548873833602.\n",
      "Epoch 39/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.6928 - mean_iou: 0.4477 - dice_coef: 0.6137 - accuracy: 0.7848\n",
      "Epoch 39: val_loss improved from 0.65260 to 0.64826, saving model to APWCS_model.h5\n",
      "18/18 [==============================] - 4s 208ms/step - loss: 0.6928 - mean_iou: 0.4477 - dice_coef: 0.6137 - accuracy: 0.7848 - val_loss: 0.6483 - val_mean_iou: 0.4737 - val_dice_coef: 0.6244 - val_accuracy: 0.8077 - lr: 3.4145e-04\n",
      "\n",
      "Epoch 40: LearningRateScheduler setting learning rate to 0.00032859932476006547.\n",
      "Epoch 40/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.6876 - mean_iou: 0.4469 - dice_coef: 0.6126 - accuracy: 0.7848\n",
      "Epoch 40: val_loss improved from 0.64826 to 0.64247, saving model to APWCS_model.h5\n",
      "18/18 [==============================] - 4s 208ms/step - loss: 0.6876 - mean_iou: 0.4469 - dice_coef: 0.6126 - accuracy: 0.7848 - val_loss: 0.6425 - val_mean_iou: 0.4766 - val_dice_coef: 0.6270 - val_accuracy: 0.8069 - lr: 3.2860e-04\n",
      "\n",
      "Epoch 41: LearningRateScheduler setting learning rate to 0.00031622776601683794.\n",
      "Epoch 41/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.6546 - mean_iou: 0.4693 - dice_coef: 0.6249 - accuracy: 0.8017\n",
      "Epoch 41: val_loss did not improve from 0.64247\n",
      "18/18 [==============================] - 3s 194ms/step - loss: 0.6546 - mean_iou: 0.4693 - dice_coef: 0.6249 - accuracy: 0.8017 - val_loss: 0.6645 - val_mean_iou: 0.4449 - val_dice_coef: 0.6220 - val_accuracy: 0.7904 - lr: 3.1623e-04\n",
      "\n",
      "Epoch 42: LearningRateScheduler setting learning rate to 0.00030432198871077214.\n",
      "Epoch 42/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.6628 - mean_iou: 0.4578 - dice_coef: 0.6280 - accuracy: 0.7998\n",
      "Epoch 42: val_loss did not improve from 0.64247\n",
      "18/18 [==============================] - 3s 191ms/step - loss: 0.6628 - mean_iou: 0.4578 - dice_coef: 0.6280 - accuracy: 0.7998 - val_loss: 0.6863 - val_mean_iou: 0.4034 - val_dice_coef: 0.6490 - val_accuracy: 0.7653 - lr: 3.0432e-04\n",
      "\n",
      "Epoch 43: LearningRateScheduler setting learning rate to 0.0002928644564625237.\n",
      "Epoch 43/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.6465 - mean_iou: 0.4829 - dice_coef: 0.6331 - accuracy: 0.8089\n",
      "Epoch 43: val_loss did not improve from 0.64247\n",
      "18/18 [==============================] - 3s 191ms/step - loss: 0.6465 - mean_iou: 0.4829 - dice_coef: 0.6331 - accuracy: 0.8089 - val_loss: 0.6824 - val_mean_iou: 0.4174 - val_dice_coef: 0.6151 - val_accuracy: 0.7772 - lr: 2.9286e-04\n",
      "\n",
      "Epoch 44: LearningRateScheduler setting learning rate to 0.0002818382931264454.\n",
      "Epoch 44/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.6890 - mean_iou: 0.4706 - dice_coef: 0.6200 - accuracy: 0.7836\n",
      "Epoch 44: val_loss improved from 0.64247 to 0.63263, saving model to APWCS_model.h5\n",
      "18/18 [==============================] - 4s 210ms/step - loss: 0.6890 - mean_iou: 0.4706 - dice_coef: 0.6200 - accuracy: 0.7836 - val_loss: 0.6326 - val_mean_iou: 0.4820 - val_dice_coef: 0.6399 - val_accuracy: 0.8018 - lr: 2.8184e-04\n",
      "\n",
      "Epoch 45: LearningRateScheduler setting learning rate to 0.00027122725793320287.\n",
      "Epoch 45/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.6570 - mean_iou: 0.4865 - dice_coef: 0.6273 - accuracy: 0.8006\n",
      "Epoch 45: val_loss improved from 0.63263 to 0.58942, saving model to APWCS_model.h5\n",
      "18/18 [==============================] - 4s 210ms/step - loss: 0.6570 - mean_iou: 0.4865 - dice_coef: 0.6273 - accuracy: 0.8006 - val_loss: 0.5894 - val_mean_iou: 0.4872 - val_dice_coef: 0.6632 - val_accuracy: 0.8194 - lr: 2.7123e-04\n",
      "\n",
      "Epoch 46: LearningRateScheduler setting learning rate to 0.00026101572156825363.\n",
      "Epoch 46/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.6370 - mean_iou: 0.4858 - dice_coef: 0.6336 - accuracy: 0.8022\n",
      "Epoch 46: val_loss did not improve from 0.58942\n",
      "18/18 [==============================] - 3s 192ms/step - loss: 0.6370 - mean_iou: 0.4858 - dice_coef: 0.6336 - accuracy: 0.8022 - val_loss: 0.6556 - val_mean_iou: 0.4616 - val_dice_coef: 0.6343 - val_accuracy: 0.7976 - lr: 2.6102e-04\n",
      "\n",
      "Epoch 47: LearningRateScheduler setting learning rate to 0.000251188643150958.\n",
      "Epoch 47/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.6152 - mean_iou: 0.4961 - dice_coef: 0.6500 - accuracy: 0.8179\n",
      "Epoch 47: val_loss improved from 0.58942 to 0.58236, saving model to APWCS_model.h5\n",
      "18/18 [==============================] - 4s 209ms/step - loss: 0.6152 - mean_iou: 0.4961 - dice_coef: 0.6500 - accuracy: 0.8179 - val_loss: 0.5824 - val_mean_iou: 0.5037 - val_dice_coef: 0.6602 - val_accuracy: 0.8286 - lr: 2.5119e-04\n",
      "\n",
      "Epoch 48: LearningRateScheduler setting learning rate to 0.0002417315480804104.\n",
      "Epoch 48/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.6448 - mean_iou: 0.4935 - dice_coef: 0.6330 - accuracy: 0.8080\n",
      "Epoch 48: val_loss did not improve from 0.58236\n",
      "18/18 [==============================] - 3s 193ms/step - loss: 0.6448 - mean_iou: 0.4935 - dice_coef: 0.6330 - accuracy: 0.8080 - val_loss: 0.6363 - val_mean_iou: 0.4782 - val_dice_coef: 0.6356 - val_accuracy: 0.8086 - lr: 2.4173e-04\n",
      "\n",
      "Epoch 49: LearningRateScheduler setting learning rate to 0.00023263050671536265.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.6446 - mean_iou: 0.4948 - dice_coef: 0.6353 - accuracy: 0.8134\n",
      "Epoch 49: val_loss did not improve from 0.58236\n",
      "18/18 [==============================] - 3s 199ms/step - loss: 0.6446 - mean_iou: 0.4948 - dice_coef: 0.6353 - accuracy: 0.8134 - val_loss: 0.6441 - val_mean_iou: 0.4292 - val_dice_coef: 0.6659 - val_accuracy: 0.7842 - lr: 2.3263e-04\n",
      "\n",
      "Epoch 50: LearningRateScheduler setting learning rate to 0.00022387211385683394.\n",
      "Epoch 50/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.6330 - mean_iou: 0.4989 - dice_coef: 0.6404 - accuracy: 0.8099\n",
      "Epoch 50: val_loss did not improve from 0.58236\n",
      "18/18 [==============================] - 3s 194ms/step - loss: 0.6330 - mean_iou: 0.4989 - dice_coef: 0.6404 - accuracy: 0.8099 - val_loss: 0.6307 - val_mean_iou: 0.5010 - val_dice_coef: 0.6303 - val_accuracy: 0.8179 - lr: 2.2387e-04\n",
      "\n",
      "Epoch 51: LearningRateScheduler setting learning rate to 0.0002154434690031884.\n",
      "Epoch 51/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.6215 - mean_iou: 0.4995 - dice_coef: 0.6473 - accuracy: 0.8163\n",
      "Epoch 51: val_loss did not improve from 0.58236\n",
      "18/18 [==============================] - 3s 194ms/step - loss: 0.6215 - mean_iou: 0.4995 - dice_coef: 0.6473 - accuracy: 0.8163 - val_loss: 0.6031 - val_mean_iou: 0.5263 - val_dice_coef: 0.6439 - val_accuracy: 0.8306 - lr: 2.1544e-04\n",
      "\n",
      "Epoch 52: LearningRateScheduler setting learning rate to 0.0002073321573485955.\n",
      "Epoch 52/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.6031 - mean_iou: 0.5037 - dice_coef: 0.6529 - accuracy: 0.8249\n",
      "Epoch 52: val_loss improved from 0.58236 to 0.58233, saving model to APWCS_model.h5\n",
      "18/18 [==============================] - 4s 206ms/step - loss: 0.6031 - mean_iou: 0.5037 - dice_coef: 0.6529 - accuracy: 0.8249 - val_loss: 0.5823 - val_mean_iou: 0.5052 - val_dice_coef: 0.6699 - val_accuracy: 0.8253 - lr: 2.0733e-04\n",
      "\n",
      "Epoch 53: LearningRateScheduler setting learning rate to 0.000199526231496888.\n",
      "Epoch 53/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.6538 - mean_iou: 0.5084 - dice_coef: 0.6303 - accuracy: 0.8054\n",
      "Epoch 53: val_loss did not improve from 0.58233\n",
      "18/18 [==============================] - 3s 191ms/step - loss: 0.6538 - mean_iou: 0.5084 - dice_coef: 0.6303 - accuracy: 0.8054 - val_loss: 0.6336 - val_mean_iou: 0.5083 - val_dice_coef: 0.6244 - val_accuracy: 0.8187 - lr: 1.9953e-04\n",
      "\n",
      "Epoch 54: LearningRateScheduler setting learning rate to 0.00019201419386388017.\n",
      "Epoch 54/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.6063 - mean_iou: 0.5010 - dice_coef: 0.6523 - accuracy: 0.8245\n",
      "Epoch 54: val_loss did not improve from 0.58233\n",
      "18/18 [==============================] - 3s 195ms/step - loss: 0.6063 - mean_iou: 0.5010 - dice_coef: 0.6523 - accuracy: 0.8245 - val_loss: 0.5863 - val_mean_iou: 0.5171 - val_dice_coef: 0.6523 - val_accuracy: 0.8299 - lr: 1.9201e-04\n",
      "\n",
      "Epoch 55: LearningRateScheduler setting learning rate to 0.00018478497974222914.\n",
      "Epoch 55/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5640 - mean_iou: 0.5240 - dice_coef: 0.6689 - accuracy: 0.8352\n",
      "Epoch 55: val_loss did not improve from 0.58233\n",
      "18/18 [==============================] - 3s 193ms/step - loss: 0.5640 - mean_iou: 0.5240 - dice_coef: 0.6689 - accuracy: 0.8352 - val_loss: 0.6047 - val_mean_iou: 0.5050 - val_dice_coef: 0.6518 - val_accuracy: 0.8212 - lr: 1.8478e-04\n",
      "\n",
      "Epoch 56: LearningRateScheduler setting learning rate to 0.0001778279410038923.\n",
      "Epoch 56/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5914 - mean_iou: 0.5133 - dice_coef: 0.6639 - accuracy: 0.8287\n",
      "Epoch 56: val_loss did not improve from 0.58233\n",
      "18/18 [==============================] - 3s 194ms/step - loss: 0.5914 - mean_iou: 0.5133 - dice_coef: 0.6639 - accuracy: 0.8287 - val_loss: 0.6026 - val_mean_iou: 0.5067 - val_dice_coef: 0.6523 - val_accuracy: 0.8228 - lr: 1.7783e-04\n",
      "\n",
      "Epoch 57: LearningRateScheduler setting learning rate to 0.00017113283041617807.\n",
      "Epoch 57/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.6189 - mean_iou: 0.5192 - dice_coef: 0.6457 - accuracy: 0.8163\n",
      "Epoch 57: val_loss improved from 0.58233 to 0.57439, saving model to APWCS_model.h5\n",
      "18/18 [==============================] - 4s 212ms/step - loss: 0.6189 - mean_iou: 0.5192 - dice_coef: 0.6457 - accuracy: 0.8163 - val_loss: 0.5744 - val_mean_iou: 0.5057 - val_dice_coef: 0.6648 - val_accuracy: 0.8275 - lr: 1.7113e-04\n",
      "\n",
      "Epoch 58: LearningRateScheduler setting learning rate to 0.00016468978654828688.\n",
      "Epoch 58/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.6023 - mean_iou: 0.5304 - dice_coef: 0.6534 - accuracy: 0.8275\n",
      "Epoch 58: val_loss improved from 0.57439 to 0.56414, saving model to APWCS_model.h5\n",
      "18/18 [==============================] - 4s 211ms/step - loss: 0.6023 - mean_iou: 0.5304 - dice_coef: 0.6534 - accuracy: 0.8275 - val_loss: 0.5641 - val_mean_iou: 0.5204 - val_dice_coef: 0.6773 - val_accuracy: 0.8330 - lr: 1.6469e-04\n",
      "\n",
      "Epoch 59: LearningRateScheduler setting learning rate to 0.00015848931924611134.\n",
      "Epoch 59/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.6351 - mean_iou: 0.5061 - dice_coef: 0.6380 - accuracy: 0.8142\n",
      "Epoch 59: val_loss improved from 0.56414 to 0.55793, saving model to APWCS_model.h5\n",
      "18/18 [==============================] - 4s 211ms/step - loss: 0.6351 - mean_iou: 0.5061 - dice_coef: 0.6380 - accuracy: 0.8142 - val_loss: 0.5579 - val_mean_iou: 0.5129 - val_dice_coef: 0.6782 - val_accuracy: 0.8305 - lr: 1.5849e-04\n",
      "\n",
      "Epoch 60: LearningRateScheduler setting learning rate to 0.0001525222956539019.\n",
      "Epoch 60/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5662 - mean_iou: 0.5239 - dice_coef: 0.6685 - accuracy: 0.8355\n",
      "Epoch 60: val_loss did not improve from 0.55793\n",
      "18/18 [==============================] - 3s 196ms/step - loss: 0.5662 - mean_iou: 0.5239 - dice_coef: 0.6685 - accuracy: 0.8355 - val_loss: 0.5683 - val_mean_iou: 0.5140 - val_dice_coef: 0.6756 - val_accuracy: 0.8278 - lr: 1.5252e-04\n",
      "\n",
      "Epoch 61: LearningRateScheduler setting learning rate to 0.00014677992676220695.\n",
      "Epoch 61/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5891 - mean_iou: 0.5372 - dice_coef: 0.6545 - accuracy: 0.8264\n",
      "Epoch 61: val_loss did not improve from 0.55793\n",
      "18/18 [==============================] - 3s 196ms/step - loss: 0.5891 - mean_iou: 0.5372 - dice_coef: 0.6545 - accuracy: 0.8264 - val_loss: 0.5716 - val_mean_iou: 0.5360 - val_dice_coef: 0.6597 - val_accuracy: 0.8399 - lr: 1.4678e-04\n",
      "\n",
      "Epoch 62: LearningRateScheduler setting learning rate to 0.00014125375446227546.\n",
      "Epoch 62/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5991 - mean_iou: 0.5260 - dice_coef: 0.6572 - accuracy: 0.8277\n",
      "Epoch 62: val_loss improved from 0.55793 to 0.54550, saving model to APWCS_model.h5\n",
      "18/18 [==============================] - 4s 209ms/step - loss: 0.5991 - mean_iou: 0.5260 - dice_coef: 0.6572 - accuracy: 0.8277 - val_loss: 0.5455 - val_mean_iou: 0.5306 - val_dice_coef: 0.6929 - val_accuracy: 0.8344 - lr: 1.4125e-04\n",
      "\n",
      "Epoch 63: LearningRateScheduler setting learning rate to 0.00013593563908785255.\n",
      "Epoch 63/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5763 - mean_iou: 0.5114 - dice_coef: 0.6681 - accuracy: 0.8332\n",
      "Epoch 63: val_loss did not improve from 0.54550\n",
      "18/18 [==============================] - 3s 194ms/step - loss: 0.5763 - mean_iou: 0.5114 - dice_coef: 0.6681 - accuracy: 0.8332 - val_loss: 0.5538 - val_mean_iou: 0.5584 - val_dice_coef: 0.6763 - val_accuracy: 0.8432 - lr: 1.3594e-04\n",
      "\n",
      "Epoch 64: LearningRateScheduler setting learning rate to 0.00013081774742601944.\n",
      "Epoch 64/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5768 - mean_iou: 0.5407 - dice_coef: 0.6672 - accuracy: 0.8329\n",
      "Epoch 64: val_loss did not improve from 0.54550\n",
      "18/18 [==============================] - 3s 193ms/step - loss: 0.5768 - mean_iou: 0.5407 - dice_coef: 0.6672 - accuracy: 0.8329 - val_loss: 0.5508 - val_mean_iou: 0.5370 - val_dice_coef: 0.6767 - val_accuracy: 0.8435 - lr: 1.3082e-04\n",
      "\n",
      "Epoch 65: LearningRateScheduler setting learning rate to 0.00012589254117941674.\n",
      "Epoch 65/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5997 - mean_iou: 0.5292 - dice_coef: 0.6512 - accuracy: 0.8299\n",
      "Epoch 65: val_loss improved from 0.54550 to 0.54147, saving model to APWCS_model.h5\n",
      "18/18 [==============================] - 4s 210ms/step - loss: 0.5997 - mean_iou: 0.5292 - dice_coef: 0.6512 - accuracy: 0.8299 - val_loss: 0.5415 - val_mean_iou: 0.5381 - val_dice_coef: 0.6844 - val_accuracy: 0.8447 - lr: 1.2589e-04\n",
      "\n",
      "Epoch 66: LearningRateScheduler setting learning rate to 0.00012115276586285887.\n",
      "Epoch 66/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5778 - mean_iou: 0.5288 - dice_coef: 0.6661 - accuracy: 0.8325\n",
      "Epoch 66: val_loss did not improve from 0.54147\n",
      "18/18 [==============================] - 3s 194ms/step - loss: 0.5778 - mean_iou: 0.5288 - dice_coef: 0.6661 - accuracy: 0.8325 - val_loss: 0.5782 - val_mean_iou: 0.5438 - val_dice_coef: 0.6530 - val_accuracy: 0.8395 - lr: 1.2115e-04\n",
      "\n",
      "Epoch 67: LearningRateScheduler setting learning rate to 0.00011659144011798318.\n",
      "Epoch 67/200\n",
      "10/18 [===============>..............] - ETA: 0s - loss: 0.5626 - mean_iou: 0.5447 - dice_coef: 0.6704 - accuracy: 0.8340"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    TrainAugmentGenerator(train_images_dir = train_images, train_masks_dir = train_masks, batch_size = batch_size, target_size = image_size), \n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data = ValAugmentGenerator(val_images_dir = val_images, val_masks_dir = val_masks, batch_size = batch_size, target_size = image_size), \n",
    "    validation_steps = validation_steps, \n",
    "    epochs = 200,\n",
    "    callbacks=callbacks,\n",
    "    use_multiprocessing=False,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d2d3f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"./APWCS_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "752a6bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_gen = ValAugmentGenerator(val_images_dir = test_images, val_masks_dir = test_masks, batch_size = batch_size, target_size = image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a6dbb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 213 images belonging to 1 classes.\n",
      "Found 213 images belonging to 1 classes.\n",
      "50/50 [==============================] - 18s 108ms/step - loss: 0.7490 - mean_iou: 0.3899 - dice_coef: 0.6283 - accuracy: 0.7303\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(\n",
    "    testing_gen,\n",
    "    batch_size = batch_size,\n",
    "    steps=50,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "07a32a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss, IoU, dice_coef, test acc: [0.29641783237457275, 0.6362375617027283, 0.8525645732879639, 0.8913767337799072]\n"
     ]
    }
   ],
   "source": [
    "print(\"test loss, IoU, dice_coef, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b90790f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 217ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(8):\n",
    "    # 배치에서 이미지와 마스크를 가져옴\n",
    "    batch_img, batch_mask = next(testing_gen)\n",
    "    pred_all = model.predict(batch_img)\n",
    "#     print(\"Predictions Shape:\", np.shape(pred_all))  # (batch_size, height, width, num_classes)\n",
    "\n",
    "    # 예측 마스크 저장\n",
    "    plt.axis('off')\n",
    "    plt.imsave('./예측사진/2%02d_Pmask.png' % i, onehot_to_rgb(pred_all[i], id2code))\n",
    "\n",
    "    # 배치 이미지를 저장할 때, 채널 축 확인\n",
    "    if batch_img.shape[-1] == 3:  # RGB 이미지인 경우\n",
    "        plt.imsave('./예측사진/2%02d_image.png' % i, batch_img[i])  # cmap 제거\n",
    "    elif batch_img.shape[-1] == 1:  # Grayscale 이미지인 경우\n",
    "        plt.imsave('./예측사진/2%02d_image.png' % i, batch_img[i].squeeze(), cmap='gray')\n",
    "\n",
    "    # 정답 마스크 저장\n",
    "    plt.imsave('./예측사진/2%02d_Tmask.png' % i, onehot_to_rgb(batch_mask[i], id2code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34903d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
